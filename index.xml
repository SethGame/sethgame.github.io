<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Home on Seth Eom</title><link>https://sethgame.github.io/</link><description>Recent content in Home on Seth Eom</description><generator>Hugo</generator><language>en</language><lastBuildDate>Sat, 02 Aug 2025 10:00:00 +0900</lastBuildDate><atom:link href="https://sethgame.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Elevator RL Multi-Agent</title><link>https://sethgame.github.io/blog/elevator-rl-multi-agent/</link><pubDate>Sat, 02 Aug 2025 10:00:00 +0900</pubDate><guid>https://sethgame.github.io/blog/elevator-rl-multi-agent/</guid><description>&lt;!-- Note: To display the image, please save it to hugo-site/static/images/ folder and uncomment the line below -->
&lt;!-- ![Elevator RL Multi-Agent Screenshot](/images/elevator-rl-multi-agent-screenshot.jpg) -->
&lt;h2 id="why">Why&lt;/h2>
&lt;ul>
&lt;li>In order to gain an understanding of more complex RL algorithms working with the FlexSim model, which includes state machine logic, multi-agent passenger management, and complex observation spaces&lt;/li>
&lt;/ul>
&lt;h2 id="how">How&lt;/h2>
&lt;ul>
&lt;li>Read and understood how the learning algorithm interfaced with the FlexSim via the state machine diagrams&lt;/li>
&lt;li>Read about how the system uses action masking (A vector to store decisions) to output decisions to act upon&lt;/li>
&lt;li>Ran the corresponding Python files to see it in action (The environment, the training, and the inference based on the trained model)&lt;/li>
&lt;li>Learned about command &lt;code>tensorboard --logdir=.&lt;/code> in order to see the Tensorboard interface, and model results without using the command palette interface&lt;/li>
&lt;li>Notice the difference in performance of the FlexSim model with the trained RL model making decisions rather than taking random decisions&lt;/li>
&lt;/ul>
&lt;h2 id="so-what">So What&lt;/h2>
&lt;ul>
&lt;li>Completing and understanding this allows for the bridging with previous more simpler concepts (From HelloWorld) to more complex agent models to understand in the future&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://grave-flood-782.notion.site/Elevator-RL-Multi-Agent-8-2-2426575c333a80e8a5cbdba481b418a8">Original Notion page&lt;/a>&lt;/p></description></item><item><title>RL hello world</title><link>https://sethgame.github.io/blog/rl-hello-world/</link><pubDate>Fri, 01 Aug 2025 10:00:00 +0900</pubDate><guid>https://sethgame.github.io/blog/rl-hello-world/</guid><description>&lt;!-- Note: To display the image, please save it to hugo-site/static/images/ folder and uncomment the line below -->
&lt;!-- ![RL Hello World Screenshot](/images/rl-hello-world-screenshot.jpg) -->
&lt;h2 id="why">Why&lt;/h2>
&lt;ul>
&lt;li>In order to gain an introductory knowledge of utilizing an RL algorithm interfacing with the FlexSim model&lt;/li>
&lt;/ul>
&lt;h2 id="how">How&lt;/h2>
&lt;ul>
&lt;li>Read the .readme text files for each of the corresponding Python files to understand how they worked&lt;/li>
&lt;li>Ran each consecutive Python file as outlined in the tutorial to see them in action&lt;/li>
&lt;li>Installed the corresponding package versions for the programs to run&lt;/li>
&lt;/ul>
&lt;h2 id="so-what">So What&lt;/h2>
&lt;ul>
&lt;li>Completing this guide on the HelloWorld allows me to engage in more complex features of this style of project, and also gain insights into using tools to help me along the way.&lt;/li>
&lt;li>For example, the use of the Tensorboard extension allows us to see the different parameter results for the training of the model&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://grave-flood-782.notion.site/RL-hello-world-8-1-2426575c333a803bb098d85c52426e08">Original Notion page&lt;/a>&lt;/p></description></item><item><title>Resume</title><link>https://sethgame.github.io/about/</link><pubDate>Wed, 21 May 2025 00:00:00 +0900</pubDate><guid>https://sethgame.github.io/about/</guid><description>&lt;h2 id="resume">Resume&lt;/h2>
&lt;p>&lt;strong>Location:&lt;/strong> Wexford, PA, USA&lt;br>
&lt;strong>LinkedIn:&lt;/strong> &lt;a href="https://linkedin.com/in/eomh">linkedin.com/in/eomh&lt;/a>&lt;/p>
&lt;h3 id="summary">Summary&lt;/h3>
&lt;p>Undergraduate Computer Engineering student at Purdue University with a passion for artificial intelligence, neural networks, and digital system design. Currently working as an undergraduate researcher focusing on AI-based image processing using PyTorch and Convolutional-Transformer approaches.&lt;/p>
&lt;h3 id="education">Education&lt;/h3>
&lt;p>&lt;strong>Purdue University&lt;/strong> | &lt;em>Bachelor of Science in Computer Engineering&lt;/em> | May 2026 (Expected)&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Location:&lt;/strong> West Lafayette, IN&lt;/li>
&lt;li>&lt;strong>GPA:&lt;/strong> 3.76&lt;/li>
&lt;li>&lt;strong>Honors:&lt;/strong> Dean&amp;rsquo;s List and Semester Honors&lt;/li>
&lt;/ul>
&lt;h3 id="skills">Skills&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Programming Languages:&lt;/strong> Python, C Programming, MATLAB, SystemVerilog, Verilog&lt;/li>
&lt;li>&lt;strong>AI/ML Technologies:&lt;/strong> PyTorch, Neural Networks, CUDA-based memory profiling&lt;/li>
&lt;li>&lt;strong>Hardware:&lt;/strong> FPGA, 3D Printing, Digital System Design, Microprocessor Systems&lt;/li>
&lt;li>&lt;strong>Tools:&lt;/strong> Excel (Graphical Analysis), Various development environments&lt;/li>
&lt;li>&lt;strong>Languages:&lt;/strong> Korean (Conversational), Japanese (Conversational), English (Native)&lt;/li>
&lt;/ul>
&lt;h3 id="experience">Experience&lt;/h3>
&lt;p>&lt;strong>Guo Lab @ Purdue ECE&lt;/strong> | &lt;em>Undergraduate Researcher&lt;/em> | December 2024 - Present&lt;/p></description></item></channel></rss>